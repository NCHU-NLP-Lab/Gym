{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yFb1PerKcRxD"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","import json\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support"]},{"cell_type":"code","source":["with open('train.json', 'r', encoding='utf-8') as file:\n","    train_data = json.load(file)\n","with open('validation.json', 'r', encoding='utf-8') as file:\n","    validation_data = json.load(file)\n","with open('test.json', 'r', encoding='utf-8') as file:\n","    test_data = json.load(file)"],"metadata":{"id":"HqkjQru0cSIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define your dataset class\n","class TranslationDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        example = self.data[idx]\n","        input_text = example[\"en\"]\n","        target_text = example[\"zh\"]\n","\n","        encoding = self.tokenizer(\n","            input_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        labels = self.tokenizer(\n","            target_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )[\"input_ids\"]\n","\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": labels.squeeze()\n","        }"],"metadata":{"id":"NFWi4kKUcSLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the mT5 model and tokenizer\n","tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"],"metadata":{"id":"I-N7cNnkcSNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets\n","train_dataset = TranslationDataset(train_data, tokenizer)\n","validation_dataset = TranslationDataset(validation_data, tokenizer)\n","test_dataset = TranslationDataset(test_data, tokenizer)"],"metadata":{"id":"GOP59Nx7cSQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**trainer**"],"metadata":{"id":"6DAWbvcXc0G7"}},{"cell_type":"code","source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./mt5_model',      # 儲存模型的位置\n","    num_train_epochs=10,           # epochs數\n","    per_device_train_batch_size=4, # train_batch_size\n","    per_device_eval_batch_size=4,  # eval_batch_size\n","    gradient_accumulation_steps=2, # 梯度累積的步數\n","    eval_accumulation_steps=2,     # 每幾步把eval_dataset從顯卡丟到cpu\n","    weight_decay=0.01,             # 權重係數\n","    logging_dir='./mt5_logs',      # 儲存log的位置\n","    logging_steps=2500,            # 每隔多少步更新一次log\n","    evaluation_strategy=\"steps\",   # 驗證的策略，steps表示按照步數驗證\n","    eval_steps=2500,               # 每隔多少步驗證一次\n","    save_total_limit=5,            # 保存的模型數量限制\n","    push_to_hub=False,             # 是否push模型到Hugging Face Hub\n",")\n","\n","# Custom data collator function\n","def data_collator(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    return {\n","        \"input_ids\": torch.stack(input_ids),\n","        \"attention_mask\": torch.stack(attention_mask),\n","        \"labels\": torch.stack(labels),\n","    }\n","from sklearn.metrics import accuracy_score\n","\n","def compute_metrics(p):\n","    predictions, labels = p.predictions, p.label_ids\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    return {\n","        'accuracy': accuracy,\n","    }\n","\n","\n","# Create Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    data_collator=data_collator,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the model\n","model.save_pretrained(\"./mt5_translation_model\")\n","tokenizer.save_pretrained(\"./mt5_translation_model\")"],"metadata":{"id":"JsIFUDPccSS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","results = trainer.evaluate(test_dataset)\n","\n","# Print the evaluation results\n","print(\"Evaluation Results:\", results)"],"metadata":{"id":"ZK4MSZkEdCcj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Seq2SeqTrainer**"],"metadata":{"id":"hUoynUBZc3jY"}},{"cell_type":"code","source":["# Define training arguments for Seq2SeqTrainer\n","training_args_seq2seq = Seq2SeqTrainingArguments(\n","    output_dir='./mt5_model_seq2seq',  # 儲存模型的位置\n","    num_train_epochs=10,               # epochs數\n","    per_device_train_batch_size=4,     # train_batch_size\n","    per_device_eval_batch_size=4,      # eval_batch_size\n","    gradient_accumulation_steps=2,     #梯度累積的步數\n","    eval_accumulation_steps=2,         #梯度累積的步數(evaluation)\n","    weight_decay=0.01,                 # 權重係數\n","    logging_dir='./mt5_logs_seq2seq',  # 儲存log的位置\n","    logging_steps=2500,                # 每隔多少步更新一次log\n","    evaluation_strategy=\"steps\",       # 驗證的策略，steps表示按照步数驗證\n","    eval_steps=2500,                   # 每隔多少步驗證一次\n","    save_total_limit=5,                # 保存的模型數量限制\n","    push_to_hub=False,                 # 是否push模型到Hugging Face Hub\n",")\n","\n","# Custom data collator function\n","def data_collator(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    return {\n","        \"input_ids\": torch.stack(input_ids),\n","        \"attention_mask\": torch.stack(attention_mask),\n","        \"labels\": torch.stack(labels),\n","    }\n","\n","# Create Seq2SeqTrainer\n","seq2seq_trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args_seq2seq,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    data_collator=data_collator,\n",")\n","\n","# Train the model using Seq2SeqTrainer\n","seq2seq_trainer.train()\n","\n","# Save the model\n","model.save_pretrained(\"./mt5_translation_model_seq2seq\")\n","tokenizer.save_pretrained(\"./mt5_translation_model_seq2seq\")"],"metadata":{"id":"8XkBqdQycqo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","seq2seq_results = seq2seq_trainer.evaluate(test_dataset)\n","\n","# Print the evaluation results\n","print(\"seq2seq_Evaluation Results:\", seq2seq_results)"],"metadata":{"id":"nMBUzRVocqjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Inference**"],"metadata":{"id":"IAZDAGXpdIlH"}},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the trained model and tokenizer\n","model = T5ForConditionalGeneration.from_pretrained(\"./mt5_translation_model\")\n","tokenizer = T5Tokenizer.from_pretrained(\"./mt5_translation_model\")\n","model_seq2seq = T5ForConditionalGeneration.from_pretrained(\"./mt5_translation_model_seq2seq\")\n","tokenizer_seq2seq = T5Tokenizer.from_pretrained(\"./mt5_translation_model_seq2seq\")\n","\n","def translate_text(input_text, model, tokenizer, max_length=512):\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n","    output_ids = model.generate(input_ids)\n","    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return translated_text"],"metadata":{"id":"_HUvGJINcxI1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**trainer**"],"metadata":{"id":"tY-tMK_vdSFe"}},{"cell_type":"code","source":["# Example inference\n","input_text = \"hello,how are you today?\"\n","translated_text = translate_text(input_text, model, tokenizer)\n","print(f\"Input Text: {input_text}\")\n","print(f\"Translated Text: {translated_text}\")"],"metadata":{"id":"cJcy32UzcxGI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Seq2SeqTrainer**"],"metadata":{"id":"fyrG-KTwdTlX"}},{"cell_type":"code","source":["# Example inference\n","input_text = \"hello,how are you today?\"\n","translated_text = translate_text(input_text, model_seq2seq, tokenizer_seq2seq)\n","print(f\"Input Text: {input_text}\")\n","print(f\"Translated Text: {translated_text}\")"],"metadata":{"id":"BOLRPkGSdN1H"},"execution_count":null,"outputs":[]}]}