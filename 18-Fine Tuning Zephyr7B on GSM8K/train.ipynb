{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 17:35:06.468884: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-07 17:35:06.487526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 17:35:06.487547: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 17:35:06.487560: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 17:35:06.491952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 17:35:06.951010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  7 17:35:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   50C    P8              36W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preload parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "new_model = \"zephyr-7b-padding\"\n",
    "output_dir = \"./logs\" # tensorboardÁµêÊûú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bitsandbytes parameters for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension (Rank)\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling (The weight matrix is scaled by ùëôùëúùëüùëé_ùëéùëôùëù‚Ñéùëé / ùëôùëúùëüùëé_ùëüùëéùëõùëò)\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainingArguments parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./logs\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (llama2 use bf16)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.2\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0} # or \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22897dd46a71426f8f12bf6117f7e31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\",'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n",
      "7473\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(len(dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_target_format(data):\n",
    "    return {'formatted_text': f\"<|user|>{data['question']}\\n<|assistant|>\\n{data['answer']}</s>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576e511d8ad247e4b0f0f493b05850dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "formatted_train_dataset = train_dataset.map(convert_to_target_format,remove_columns=[\"question\",\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'formatted_text': '<|user|>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n<|assistant|>\\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(formatted_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Ali had $21. Leila gave him half of her $100. How much does Ali have now?\n",
      "\n",
      "answer: Leila gave 100/2 = $<<100/2=50>>50 to Ali.\n",
      "Ali now has $21+ $50 = $<<21+50=71>>71.\n",
      "#### 71\n",
      "\n",
      "----model_inferencing----\n",
      "<|user|>Ali had $21. Leila gave him half of her $100. How much does Ali have now?</s>\n",
      "<|assistant|>\n",
      "After Leila gives Ali half of her $100, Ali now has half of his original $21, plus the $50 he received from Leila.\n",
      "\n",
      "Let's call Ali's original $21 as x.\n",
      "\n",
      "Leila gives Ali half of her $100, which is $50.\n",
      "\n",
      "Ali's new amount = x + 50\n",
      "\n",
      "Now, we know that Leila's new amount is $50 less than her original $100.\n",
      "\n",
      "Leila's new amount = $50\n",
      "\n",
      "Leila's original amount - Leila's new amount = $100 - $50\n",
      "\n",
      "100 - Leila's new amount = x\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "data_list = list(dataset['test'])  # ÂÅáËÆæÊÇ®ÊÉ≥Ë¶Å‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÊäΩÂèñÊï∞ÊçÆ\n",
    "# ‰ªéÂàóË°®‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏ÄÊù°Êï∞ÊçÆ\n",
    "random_data_point = random.choice(data_list)\n",
    "print(f\"question: {random_data_point['question']}\\n\")\n",
    "print(f\"answer: {random_data_point['answer']}\\n\")\n",
    "# Áé∞Âú® random_data_point ÂåÖÂê´‰∫ÜÊÇ®ÈöèÊú∫ÈÄâÊã©ÁöÑ‰∏ÄÊù°Êï∞ÊçÆ\n",
    "# print(random_data_point['question'])\n",
    "print(\"----model_inferencing----\")\n",
    "\n",
    "prompt = random_data_point['question']\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<|user|>{prompt}</s>\\n\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93f72a7ef07434ab741215260509d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    resume_from_checkpoint=True,\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"formatted_text\", # Ëá™Â∑±ËôïÁêÜÂæåÁöÑÊ¨Ñ‰Ωç\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5092, 'learning_rate': 1.1061946902654869e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8316, 'learning_rate': 2.2123893805309738e-05, 'epoch': 0.01}\n",
      "{'loss': 1.1951, 'learning_rate': 3.3185840707964604e-05, 'epoch': 0.02}\n",
      "{'loss': 1.1891, 'learning_rate': 4.4247787610619477e-05, 'epoch': 0.03}\n",
      "{'loss': 0.8613, 'learning_rate': 4.999864732969518e-05, 'epoch': 0.03}\n",
      "{'loss': 0.8681, 'learning_rate': 4.99871412250638e-05, 'epoch': 0.04}\n",
      "{'loss': 0.8493, 'learning_rate': 4.9963899583683975e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8803, 'learning_rate': 4.9928933321295676e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8425, 'learning_rate': 4.9882258860260065e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7904, 'learning_rate': 4.982389812184646e-05, 'epoch': 0.07}\n",
      "{'loss': 0.8548, 'learning_rate': 4.975387851593677e-05, 'epoch': 0.07}\n",
      "{'loss': 0.8691, 'learning_rate': 4.9672232928152096e-05, 'epoch': 0.08}\n",
      "{'loss': 0.804, 'learning_rate': 4.957899970440752e-05, 'epoch': 0.09}\n",
      "{'loss': 0.8556, 'learning_rate': 4.9474222632902484e-05, 'epoch': 0.09}\n",
      "{'loss': 0.8056, 'learning_rate': 4.935795092355508e-05, 'epoch': 0.1}\n",
      "{'loss': 0.8183, 'learning_rate': 4.923023918488997e-05, 'epoch': 0.11}\n",
      "{'loss': 0.8064, 'learning_rate': 4.909114739839079e-05, 'epoch': 0.11}\n",
      "{'loss': 0.8196, 'learning_rate': 4.894074089032904e-05, 'epoch': 0.12}\n",
      "{'loss': 0.8207, 'learning_rate': 4.877909030108277e-05, 'epoch': 0.13}\n",
      "{'loss': 0.7834, 'learning_rate': 4.86062715519594e-05, 'epoch': 0.13}\n",
      "{'loss': 0.778, 'learning_rate': 4.84223658095383e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7855, 'learning_rate': 4.822745944754981e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8076, 'learning_rate': 4.802164400630873e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7936, 'learning_rate': 4.7805016149721196e-05, 'epoch': 0.16}\n",
      "{'loss': 0.839, 'learning_rate': 4.7577677619885234e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7601, 'learning_rate': 4.733973518930623e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7699, 'learning_rate': 4.709130061074987e-05, 'epoch': 0.18}\n",
      "{'loss': 0.827, 'learning_rate': 4.683249056475593e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7705, 'learning_rate': 4.656342660483782e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7792, 'learning_rate': 4.628423510039335e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7321, 'learning_rate': 4.5995047177353654e-05, 'epoch': 0.21}\n",
      "{'loss': 0.7519, 'learning_rate': 4.569599865659828e-05, 'epoch': 0.21}\n",
      "{'loss': 0.7887, 'learning_rate': 4.5387229990165073e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8502, 'learning_rate': 4.5068886195285045e-05, 'epoch': 0.23}\n",
      "{'loss': 0.7948, 'learning_rate': 4.474111678627318e-05, 'epoch': 0.23}\n",
      "{'loss': 0.7786, 'learning_rate': 4.4404075704307044e-05, 'epoch': 0.24}\n",
      "{'loss': 0.7327, 'learning_rate': 4.4057921245126356e-05, 'epoch': 0.25}\n",
      "{'loss': 0.8027, 'learning_rate': 4.370281598468725e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7899, 'learning_rate': 4.333892670280645e-05, 'epoch': 0.26}\n",
      "{'loss': 0.7952, 'learning_rate': 4.296642430483087e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7997, 'learning_rate': 4.2585483741369755e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7641, 'learning_rate': 4.21962839261268e-05, 'epoch': 0.28}\n",
      "{'loss': 0.782, 'learning_rate': 4.179900765187102e-05, 'epoch': 0.29}\n",
      "{'loss': 0.8018, 'learning_rate': 4.139384150458575e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7205, 'learning_rate': 4.098097577583605e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7995, 'learning_rate': 4.0560604373395864e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7372, 'learning_rate': 4.013292473017666e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7843, 'learning_rate': 3.969813771150049e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7801, 'learning_rate': 3.925644752076101e-05, 'epoch': 0.33}\n",
      "{'loss': 0.8047, 'learning_rate': 3.880806160351663e-05, 'epoch': 0.33}\n",
      "{'loss': 0.7866, 'learning_rate': 3.835319055006106e-05, 'epoch': 0.34}\n",
      "{'loss': 0.7644, 'learning_rate': 3.7892047996516784e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6981, 'learning_rate': 3.742485052449812e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7869, 'learning_rate': 3.695181755939079e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7963, 'learning_rate': 3.647317126729596e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7897, 'learning_rate': 3.598913645068703e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7519, 'learning_rate': 3.5499940442828206e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7561, 'learning_rate': 3.500581300100447e-05, 'epoch': 0.39}\n",
      "{'loss': 0.7646, 'learning_rate': 3.450698619861308e-05, 'epoch': 0.39}\n",
      "{'loss': 0.8038, 'learning_rate': 3.400369431616727e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7777, 'learning_rate': 3.3496173731263377e-05, 'epoch': 0.41}\n",
      "{'loss': 0.8173, 'learning_rate': 3.298466280756295e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7732, 'learning_rate': 3.246940178284224e-05, 'epoch': 0.42}\n",
      "{'loss': 0.8052, 'learning_rate': 3.1950632656161336e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7365, 'learning_rate': 3.142859907420615e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7931, 'learning_rate': 3.09035462168566e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7947, 'learning_rate': 3.037572068203466e-05, 'epoch': 0.45}\n",
      "{'loss': 0.8267, 'learning_rate': 2.984537036988644e-05, 'epoch': 0.45}\n",
      "{'loss': 0.7323, 'learning_rate': 2.931274436635266e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8189, 'learning_rate': 2.8778092826182174e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7976, 'learning_rate': 2.824166685544353e-05, 'epoch': 0.47}\n",
      "{'loss': 0.767, 'learning_rate': 2.770371839358973e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7891, 'learning_rate': 2.716450009513158e-05, 'epoch': 0.49}\n",
      "{'loss': 0.768, 'learning_rate': 2.662426521097523e-05, 'epoch': 0.5}\n",
      "{'loss': 0.76, 'learning_rate': 2.6083267469479545e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7988, 'learning_rate': 2.554176095728928e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7559, 'learning_rate': 2.5e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7768, 'learning_rate': 2.4458239042710727e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7883, 'learning_rate': 2.391673253052046e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7495, 'learning_rate': 2.3375734789024774e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7587, 'learning_rate': 2.283549990486842e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7791, 'learning_rate': 2.229628160641028e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8223, 'learning_rate': 2.1758333144556474e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7863, 'learning_rate': 2.1221907173817832e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7708, 'learning_rate': 2.0687255633647348e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8032, 'learning_rate': 2.015462963011357e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7382, 'learning_rate': 1.9624279317965354e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7675, 'learning_rate': 1.9096453783143408e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6872, 'learning_rate': 1.8571400925793855e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7874, 'learning_rate': 1.8049367343838663e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7181, 'learning_rate': 1.7530598217157756e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7873, 'learning_rate': 1.7015337192437054e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7716, 'learning_rate': 1.6503826268736633e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7694, 'learning_rate': 1.5996305683832736e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7415, 'learning_rate': 1.5493013801386926e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7549, 'learning_rate': 1.4994186998995535e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7687, 'learning_rate': 1.4500059557171791e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7508, 'learning_rate': 1.4010863549312969e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7937, 'learning_rate': 1.3526828732704042e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7545, 'learning_rate': 1.304818244060922e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7468, 'learning_rate': 1.257514947550189e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8053, 'learning_rate': 1.2107952003483217e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7954, 'learning_rate': 1.1646809449938944e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7814, 'learning_rate': 1.119193839648337e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7209, 'learning_rate': 1.0743552479238994e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7806, 'learning_rate': 1.0301862288499503e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7488, 'learning_rate': 9.867075269823352e-06, 'epoch': 0.72}\n",
      "{'loss': 0.7985, 'learning_rate': 9.43939562660415e-06, 'epoch': 0.72}\n",
      "{'loss': 0.7313, 'learning_rate': 9.019024224163954e-06, 'epoch': 0.73}\n",
      "{'loss': 0.7507, 'learning_rate': 8.606158495414257e-06, 'epoch': 0.74}\n",
      "{'loss': 0.7591, 'learning_rate': 8.200992348128977e-06, 'epoch': 0.74}\n",
      "{'loss': 0.762, 'learning_rate': 7.803716073873204e-06, 'epoch': 0.75}\n",
      "{'loss': 0.7712, 'learning_rate': 7.414516258630244e-06, 'epoch': 0.76}\n",
      "{'loss': 0.7906, 'learning_rate': 7.033575695169131e-06, 'epoch': 0.76}\n",
      "{'loss': 0.7481, 'learning_rate': 6.661073297193557e-06, 'epoch': 0.77}\n",
      "{'loss': 0.7587, 'learning_rate': 6.297184015312754e-06, 'epoch': 0.78}\n",
      "{'loss': 0.7848, 'learning_rate': 5.9420787548736535e-06, 'epoch': 0.78}\n",
      "{'loss': 0.7417, 'learning_rate': 5.595924295692951e-06, 'epoch': 0.79}\n",
      "{'loss': 0.7737, 'learning_rate': 5.2588832137268285e-06, 'epoch': 0.8}\n",
      "{'loss': 0.7766, 'learning_rate': 4.9311138047149566e-06, 'epoch': 0.8}\n",
      "{'loss': 0.7975, 'learning_rate': 4.61277000983493e-06, 'epoch': 0.81}\n",
      "{'loss': 0.8425, 'learning_rate': 4.30400134340172e-06, 'epoch': 0.82}\n",
      "{'loss': 0.7793, 'learning_rate': 4.004952822646349e-06, 'epoch': 0.82}\n",
      "{'loss': 0.7523, 'learning_rate': 3.7157648996066635e-06, 'epoch': 0.83}\n",
      "{'loss': 0.7265, 'learning_rate': 3.4365733951621793e-06, 'epoch': 0.84}\n",
      "{'loss': 0.7814, 'learning_rate': 3.167509435244073e-06, 'epoch': 0.84}\n",
      "{'loss': 0.7483, 'learning_rate': 2.9086993892501386e-06, 'epoch': 0.85}\n",
      "{'loss': 0.7753, 'learning_rate': 2.6602648106937714e-06, 'epoch': 0.86}\n",
      "{'loss': 0.7848, 'learning_rate': 2.422322380114772e-06, 'epoch': 0.86}\n",
      "{'loss': 0.7543, 'learning_rate': 2.1949838502788032e-06, 'epoch': 0.87}\n",
      "{'loss': 0.7307, 'learning_rate': 1.9783559936912775e-06, 'epoch': 0.88}\n",
      "{'loss': 0.8071, 'learning_rate': 1.7725405524501943e-06, 'epoch': 0.88}\n",
      "{'loss': 0.7714, 'learning_rate': 1.5776341904617048e-06, 'epoch': 0.89}\n",
      "{'loss': 0.7538, 'learning_rate': 1.3937284480405987e-06, 'epoch': 0.9}\n",
      "{'loss': 0.769, 'learning_rate': 1.2209096989172357e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7645, 'learning_rate': 1.0592591096709653e-06, 'epoch': 0.91}\n",
      "{'loss': 0.7441, 'learning_rate': 9.088526016092142e-07, 'epoch': 0.92}\n",
      "{'loss': 0.7411, 'learning_rate': 7.697608151100322e-07, 'epoch': 0.92}\n",
      "{'loss': 0.8004, 'learning_rate': 6.420490764449228e-07, 'epoch': 0.93}\n",
      "{'loss': 0.7761, 'learning_rate': 5.257773670975213e-07, 'epoch': 0.94}\n",
      "{'loss': 0.7557, 'learning_rate': 4.21000295592483e-07, 'epoch': 0.94}\n",
      "{'loss': 0.7742, 'learning_rate': 3.2776707184790644e-07, 'epoch': 0.95}\n",
      "{'loss': 0.7738, 'learning_rate': 2.461214840632331e-07, 'epoch': 0.96}\n",
      "{'loss': 0.7402, 'learning_rate': 1.7610187815355061e-07, 'epoch': 0.96}\n",
      "{'loss': 0.7889, 'learning_rate': 1.1774113973994083e-07, 'epoch': 0.97}\n",
      "{'loss': 0.7673, 'learning_rate': 7.106667870432071e-08, 'epoch': 0.98}\n",
      "{'loss': 0.7307, 'learning_rate': 3.610041631602612e-08, 'epoch': 0.98}\n",
      "{'loss': 0.7948, 'learning_rate': 1.285877493619747e-08, 'epoch': 0.99}\n",
      "{'loss': 0.7571, 'learning_rate': 1.3526703048216682e-09, 'epoch': 1.0}\n",
      "{'train_runtime': 902.0186, 'train_samples_per_second': 8.285, 'train_steps_per_second': 4.143, 'train_loss': 0.797121823519859, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3737, training_loss=0.797121823519859, metrics={'train_runtime': 902.0186, 'train_samples_per_second': 8.285, 'train_steps_per_second': 4.143, 'train_loss': 0.797121823519859, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "# del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload model in FP16 and merge it with LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0190066d729a4ae8a8c8e28ca3f6c425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zephyr-7b-padding-full/tokenizer_config.json',\n",
       " 'zephyr-7b-padding-full/special_tokens_map.json',\n",
       " 'zephyr-7b-padding-full/tokenizer.model',\n",
       " 'zephyr-7b-padding-full/added_tokens.json',\n",
       " 'zephyr-7b-padding-full/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(new_model+\"-full\") # ÂÑ≤Â≠òÂÆåÊï¥Ê®°Âûã\n",
    "tokenizer.save_pretrained(new_model+\"-full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b53e292369b4749ba1df508677126e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39mset_verbosity(logging\u001b[38;5;241m.\u001b[39mCRITICAL)\n\u001b[1;32m      3\u001b[0m inf_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./zephyr-7b-padding-full\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m inf_model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m inf_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(inf_model_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3694\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3686\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3687\u001b[0m     (\n\u001b[1;32m   3688\u001b[0m         model,\n\u001b[1;32m   3689\u001b[0m         missing_keys,\n\u001b[1;32m   3690\u001b[0m         unexpected_keys,\n\u001b[1;32m   3691\u001b[0m         mismatched_keys,\n\u001b[1;32m   3692\u001b[0m         offload_index,\n\u001b[1;32m   3693\u001b[0m         error_msgs,\n\u001b[0;32m-> 3694\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3712\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3713\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                     )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4120\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:755\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 755\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "inf_model_path=\"./zephyr-7b-padding-full\"\n",
    "inf_model=AutoModelForCausalLM.from_pretrained(inf_model_path, device_map=\"auto\")\n",
    "inf_tokenizer = AutoTokenizer.from_pretrained(inf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James decides to buy birthday candles for his 2 sons.  One of them is 12 and the other is 4 years younger.  A pack of 5 candles costs $3.  How much does James spend on candles?\n",
      "<|assistant|>\n",
      "The older son is 12 so the younger son is 12-4=<<12-4=8>>8 years old.\n",
      "The older son needs 12 candles and the younger son needs 8 candles so they need 12+8=<<12+8=20>>20 candles.\n",
      "20 candles cost $3 each so they cost 20*3=$<<20*3=60>>60.\n",
      "#### 10. A store sells 1000 pairs of shoes.  The store sells 200 pairs of shoes on Monday, 300\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "import random\n",
    "# Â∞ÜÊï∞ÊçÆÈõÜËΩ¨Êç¢‰∏∫ Python ÂàóË°®\n",
    "data_list = list(dataset['test'])  # ÂÅáËÆæÊÇ®ÊÉ≥Ë¶Å‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÊäΩÂèñÊï∞ÊçÆ\n",
    "\n",
    "# ‰ªéÂàóË°®‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏ÄÊù°Êï∞ÊçÆ\n",
    "random_data_point = random.choice(data_list)\n",
    "\n",
    "# Áé∞Âú® random_data_point ÂåÖÂê´‰∫ÜÊÇ®ÈöèÊú∫ÈÄâÊã©ÁöÑ‰∏ÄÊù°Êï∞ÊçÆ\n",
    "# print(random_data_point['question'])\n",
    "\n",
    "\n",
    "prompt = random_data_point['question']\n",
    "pipe = pipeline(task=\"text-generation\", model=inf_model, tokenizer=inf_tokenizer, max_length=200)\n",
    "result = pipe(f\"<|user|>{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
